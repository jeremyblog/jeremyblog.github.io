---
layout: post
title: zookeeper实现高并发的自增ID
---
自增ID在数据维护的场景非常多，比如在大型的IM系统中，每个用户都被分配一个独立的id区段，每次心跳的时候都附带这个最新值id，服务端再根据这个id在心跳中返回是否有新的未更新数据。
<br>
这个场景在有几千万在线人数的产品，并发请求是非常高的，每条消息都需要一个新的id，高峰时tps甚至达到几万。
<br>
这个场景要求:
<br>
1. 保证生成的ID唯一
2. 支持高并发请求id
3. 每个用户都应该有自己的id
4. id不能大于64bits
5. 生成id的速度有要求，需要每秒生成上几万个id
<br>

### 设计迭代

**第一版，每个用户都分配一个id段，而不是统一的id端**

原因很简单，一般id的长度是64bit，这虽然挺大，但是对于一个承载几千万用户的，无限增长的聊天消息的系统来说，不久的将来这个值就不够用了。另外是一个性能上的考虑。
<br>
为了提高并发请求的能力，以上的数据不能放在任何的数据库中，而是放到内存里。即便如此，一秒几万的请求，单纯乐观锁这个代价也极其大，造成大量的请求被堵塞，带来严重的性能问题。
<br>
每个用户有自己独立的id区间，这将并发问题弱化为简单的数据库操作了，不过设计上还是有一个非常大的问题，就是哪怕是针对内存DB的乐观锁操作，在海量并发的场景仍然有很大的问题。
<br>
这个版本在线上高峰时，系统的曾经出现大量请求超时的问题。
<br>

![](http://oq6i0apwz.bkt.clouddn.com/seq_v1.png)

**第二版，每次从服务器获取一小块id段，而不是一个**

这个设计参考tim在新浪博客上的设计，当初他使用redis来实现自增id就是采用了这个方法，每次都从redis获取一小块的id段，用完了才重新由服务器获取。
<br>

![](http://oq6i0apwz.bkt.clouddn.com/seq_v2.png)
<br>
在这之前，客户端不用再频繁获取id了，不过设计带来了一个问题就是初始化的时候，需要加载大量用户的设置，这会让启动变得非常的慢。

**第三版，共享id段**

这个参考了微信关于序列生成器的设计，当初遇到问题的时候，没有大胆地如此设想，重要的是需要修改的地方比较多。
<br>

![](http://oq6i0apwz.bkt.clouddn.com/seq_v3.png)

这里做了一些修改，因为我们的用户有多重身份，每个身份仅会出现在一个社交圈中，所以我们使用圈id代替user id。让圈内的用户共享一个max id即可。
<br>

**解决分布式问题的最佳手段就是不要分布式**

这是很深的体会，id的服务请求是毫秒设置是微妙级别的，一秒处理几万的请求是绰绰有余的。回头看看我们的需求，应该全部满足了，除了容灾性。
<br>
使用zookeeper的竞争锁可以轻易满足的。